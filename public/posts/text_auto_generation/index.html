<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>텍스트 자동 완성 기능 구현: 자연어 처리 실습 | 펀앱스</title>
<meta name="keywords" content="텍스트 자동 완성, 자연어 처리, Python, 기계 학습, NLP">
<meta name="description" content="텍스트 자동 완성 기능은 사용자가 입력하는 단어나 문장을 예측하여 자동으로 완성하는 기능입니다. 이번 글에서는 Python과 자연어 처리(NLP) 기술을 사용하여 간단한 텍스트 자동 완성 기능을 구현하는 방법을 소개하겠습니다. 이 가이드는 초보자도 쉽게 따라할 수 있도록 구성되어 있습니다.
준비 작업 Python과 필요한 라이브러리 설치하기 우선 Python과 몇 가지 주요 라이브러리를 설치해야 합니다. 다음 명령어를 사용하여 필요한 패키지를 설치할 수 있습니다:
pip install numpy pandas tensorflow keras nltk 데이터 준비 텍스트 자동 완성 모델을 학습시키기 위해서는 대량의 텍스트 데이터가 필요합니다.">
<meta name="author" content="">
<link rel="canonical" href="https://funapps.site/posts/text_auto_generation/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.54405a410796490bc874ab6181fac9b675753cc2b91375d8f882566459eca428.css" integrity="sha256-VEBaQQeWSQvIdKthgfrJtnV1PMK5E3XY&#43;IJWZFnspCg=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://funapps.site/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://funapps.site/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://funapps.site/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://funapps.site/apple-touch-icon.png">
<link rel="mask-icon" href="https://funapps.site/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://funapps.site/posts/text_auto_generation/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<meta name="google-adsense-account" content="ca-pub-5886736974219758">
<meta name="naver-site-verification" content="0dac8dfc2cbdfd3ff234200a86d7af57f719b966" />
  

<meta property="og:title" content="텍스트 자동 완성 기능 구현: 자연어 처리 실습" />
<meta property="og:description" content="텍스트 자동 완성 기능은 사용자가 입력하는 단어나 문장을 예측하여 자동으로 완성하는 기능입니다. 이번 글에서는 Python과 자연어 처리(NLP) 기술을 사용하여 간단한 텍스트 자동 완성 기능을 구현하는 방법을 소개하겠습니다. 이 가이드는 초보자도 쉽게 따라할 수 있도록 구성되어 있습니다.
준비 작업 Python과 필요한 라이브러리 설치하기 우선 Python과 몇 가지 주요 라이브러리를 설치해야 합니다. 다음 명령어를 사용하여 필요한 패키지를 설치할 수 있습니다:
pip install numpy pandas tensorflow keras nltk 데이터 준비 텍스트 자동 완성 모델을 학습시키기 위해서는 대량의 텍스트 데이터가 필요합니다." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://funapps.site/posts/text_auto_generation/" />
<meta property="og:image" content="https://funapps.site/images/ai14.webp" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-07-27T20:16:38+12:00" />
<meta property="article:modified_time" content="2024-07-27T20:16:38+12:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://funapps.site/images/ai14.webp" />
<meta name="twitter:title" content="텍스트 자동 완성 기능 구현: 자연어 처리 실습"/>
<meta name="twitter:description" content="텍스트 자동 완성 기능은 사용자가 입력하는 단어나 문장을 예측하여 자동으로 완성하는 기능입니다. 이번 글에서는 Python과 자연어 처리(NLP) 기술을 사용하여 간단한 텍스트 자동 완성 기능을 구현하는 방법을 소개하겠습니다. 이 가이드는 초보자도 쉽게 따라할 수 있도록 구성되어 있습니다.
준비 작업 Python과 필요한 라이브러리 설치하기 우선 Python과 몇 가지 주요 라이브러리를 설치해야 합니다. 다음 명령어를 사용하여 필요한 패키지를 설치할 수 있습니다:
pip install numpy pandas tensorflow keras nltk 데이터 준비 텍스트 자동 완성 모델을 학습시키기 위해서는 대량의 텍스트 데이터가 필요합니다."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://funapps.site/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "텍스트 자동 완성 기능 구현: 자연어 처리 실습",
      "item": "https://funapps.site/posts/text_auto_generation/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "텍스트 자동 완성 기능 구현: 자연어 처리 실습",
  "name": "텍스트 자동 완성 기능 구현: 자연어 처리 실습",
  "description": "텍스트 자동 완성 기능은 사용자가 입력하는 단어나 문장을 예측하여 자동으로 완성하는 기능입니다. 이번 글에서는 Python과 자연어 처리(NLP) 기술을 사용하여 간단한 텍스트 자동 완성 기능을 구현하는 방법을 소개하겠습니다. 이 가이드는 초보자도 쉽게 따라할 수 있도록 구성되어 있습니다.\n준비 작업 Python과 필요한 라이브러리 설치하기 우선 Python과 몇 가지 주요 라이브러리를 설치해야 합니다. 다음 명령어를 사용하여 필요한 패키지를 설치할 수 있습니다:\npip install numpy pandas tensorflow keras nltk 데이터 준비 텍스트 자동 완성 모델을 학습시키기 위해서는 대량의 텍스트 데이터가 필요합니다.",
  "keywords": [
    "텍스트 자동 완성", "자연어 처리", "Python", "기계 학습", "NLP"
  ],
  "articleBody": "텍스트 자동 완성 기능은 사용자가 입력하는 단어나 문장을 예측하여 자동으로 완성하는 기능입니다. 이번 글에서는 Python과 자연어 처리(NLP) 기술을 사용하여 간단한 텍스트 자동 완성 기능을 구현하는 방법을 소개하겠습니다. 이 가이드는 초보자도 쉽게 따라할 수 있도록 구성되어 있습니다.\n준비 작업 Python과 필요한 라이브러리 설치하기 우선 Python과 몇 가지 주요 라이브러리를 설치해야 합니다. 다음 명령어를 사용하여 필요한 패키지를 설치할 수 있습니다:\npip install numpy pandas tensorflow keras nltk 데이터 준비 텍스트 자동 완성 모델을 학습시키기 위해서는 대량의 텍스트 데이터가 필요합니다. 이번 예제에서는 NLTK 라이브러리에서 제공하는 셰익스피어의 작품을 사용하겠습니다:\nimport nltk nltk.download('gutenberg') from nltk.corpus import gutenberg texts = gutenberg.raw('shakespeare-hamlet.txt') 데이터 전처리 데이터를 전처리하여 모델 학습에 적합한 형태로 변환합니다. 여기에는 텍스트 토큰화와 시퀀스 생성이 포함됩니다:\nfrom tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences # 텍스트 토큰화 tokenizer = Tokenizer() tokenizer.fit_on_texts([texts]) total_words = len(tokenizer.word_index) + 1 # 시퀀스 생성 input_sequences = [] for line in texts.split('\\n'): token_list = tokenizer.texts_to_sequences([line])[0] for i in range(1, len(token_list)): n_gram_sequence = token_list[:i+1] input_sequences.append(n_gram_sequence) # 시퀀스 패딩 max_sequence_len = max([len(x) for x in input_sequences]) input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre') # 특징과 레이블 분리 import numpy as np input_sequences = np.array(input_sequences) X, y = input_sequences[:,:-1], input_sequences[:,-1] # 레이블 원핫 인코딩 y = tf.keras.utils.to_categorical(y, num_classes=total_words) 모델 구성 간단한 신경망 모델을 사용하여 텍스트 자동 완성 모델을 구성합니다:\nfrom tensorflow.keras.models import Sequential from tensorflow.keras.layers import Embedding, LSTM, Dense model = Sequential([ Embedding(total_words, 64, input_length=max_sequence_len-1), LSTM(100), Dense(total_words, activation='softmax') ]) model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) 모델 훈련 모델을 훈련시킵니다:\nhistory = model.fit(X, y, epochs=100, verbose=1) 텍스트 자동 완성 함수 훈련된 모델을 사용하여 텍스트 자동 완성 기능을 구현합니다:\ndef predict_next_word(model, tokenizer, text, max_sequence_len): token_list = tokenizer.texts_to_sequences([text])[0] token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre') predicted = model.predict(token_list, verbose=0) predicted_word_index = np.argmax(predicted, axis=1) predicted_word = tokenizer.index_word[predicted_word_index[0]] return predicted_word seed_text = \"To be or not to be\" next_word = predict_next_word(model, tokenizer, seed_text, max_sequence_len) print(f\"Seed text: '{seed_text}', Next word: '{next_word}'\") 전체 코드 예제 아래는 위의 모든 단계를 포함한 전체 코드 예제입니다:\nimport nltk nltk.download('gutenberg') from nltk.corpus import gutenberg import numpy as np import tensorflow as tf from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Embedding, LSTM, Dense # 데이터 준비 texts = gutenberg.raw('shakespeare-hamlet.txt') # 텍스트 토큰화 tokenizer = Tokenizer() tokenizer.fit_on_texts([texts]) total_words = len(tokenizer.word_index) + 1 # 시퀀스 생성 input_sequences = [] for line in texts.split('\\n'): token_list = tokenizer.texts_to_sequences([line])[0] for i in range(1, len(token_list)): n_gram_sequence = token_list[:i+1] input_sequences.append(n_gram_sequence) # 시퀀스 패딩 max_sequence_len = max([len(x) for x in input_sequences]) input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre') # 특징과 레이블 분리 input_sequences = np.array(input_sequences) X, y = input_sequences[:,:-1], input_sequences[:,-1] # 레이블 원핫 인코딩 y = tf.keras.utils.to_categorical(y, num_classes=total_words) # 신경망 모델 구성 model = Sequential([ Embedding(total_words, 64, input_length=max_sequence_len-1), LSTM(100), Dense(total_words, activation='softmax') ]) model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # 모델 훈련 history = model.fit(X, y, epochs=100, verbose=1) # 텍스트 자동 완성 함수 def predict_next_word(model, tokenizer, text, max_sequence_len): token_list = tokenizer.texts_to_sequences([text])[0] token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre') predicted = model.predict(token_list, verbose=0) predicted_word_index = np.argmax(predicted, axis=1) predicted_word = tokenizer.index_word[predicted_word_index[0]] return predicted_word seed_text = \"To be or not to be\" next_word = predict_next_word(model, tokenizer, seed_text, max_sequence_len) print(f\"Seed text: '{seed_text}', Next word: '{next_word}'\") 이 코드는 간단한 텍스트 자동 완성 기능의 기본적인 구조를 제공합니다. 실제 기능의 성능을 높이기 위해서는 다양한 데이터 전처리 기법과 모델을 적용할 수 있습니다.\n마무리 이번 글에서는 Python과 자연어 처리를 사용하여 간단한 텍스트 자동 완성 기능을 구현하는 방법을 소개했습니다. 텍스트 자동 완성 기능은 다양한 애플리케이션에서 유용하게 사용될 수 있습니다. 다음 포스트에서는 조금 더 심화된 내용인 ‘Python과 OpenCV를 이용한 실시간 객체 추적’을 다뤄보도록 하겠습니다.\n",
  "wordCount" : "551",
  "inLanguage": "en",
  "image":"https://funapps.site/images/ai14.webp","datePublished": "2024-07-27T20:16:38+12:00",
  "dateModified": "2024-07-27T20:16:38+12:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://funapps.site/posts/text_auto_generation/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "펀앱스",
    "logo": {
      "@type": "ImageObject",
      "url": "https://funapps.site/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://funapps.site/" accesskey="h" title="펀앱스 (Alt + H)">펀앱스</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://funapps.site/archives/" title="아카이브">
                    <span>아카이브</span>
                </a>
            </li>
            <li>
                <a href="https://funapps.site/search/" title="검색 (Alt &#43; /)" accesskey=/>
                    <span>검색</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://funapps.site/">Home</a>&nbsp;»&nbsp;<a href="https://funapps.site/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      텍스트 자동 완성 기능 구현: 자연어 처리 실습
    </h1>
    <div class="post-meta"><span title='2024-07-27 20:16:38 +1200 NZST'>July 27, 2024</span>

</div>
  </header> 
<figure class="entry-cover"><img loading="eager" src="https://funapps.site/images/ai14.webp" alt="AI Deep Learning Model">
        
</figure>
  <div class="post-content"><p>텍스트 자동 완성 기능은 사용자가 입력하는 단어나 문장을 예측하여 자동으로 완성하는 기능입니다. 이번 글에서는 Python과 자연어 처리(NLP) 기술을 사용하여 간단한 텍스트 자동 완성 기능을 구현하는 방법을 소개하겠습니다. 이 가이드는 초보자도 쉽게 따라할 수 있도록 구성되어 있습니다.</p>
<h2 id="준비-작업">준비 작업<a hidden class="anchor" aria-hidden="true" href="#준비-작업">#</a></h2>
<h3 id="python과-필요한-라이브러리-설치하기">Python과 필요한 라이브러리 설치하기<a hidden class="anchor" aria-hidden="true" href="#python과-필요한-라이브러리-설치하기">#</a></h3>
<p>우선 Python과 몇 가지 주요 라이브러리를 설치해야 합니다. 다음 명령어를 사용하여 필요한 패키지를 설치할 수 있습니다:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip install numpy pandas tensorflow keras nltk
</span></span></code></pre></div><h2 id="데이터-준비">데이터 준비<a hidden class="anchor" aria-hidden="true" href="#데이터-준비">#</a></h2>
<p>텍스트 자동 완성 모델을 학습시키기 위해서는 대량의 텍스트 데이터가 필요합니다. 이번 예제에서는 NLTK 라이브러리에서 제공하는 셰익스피어의 작품을 사용하겠습니다:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> nltk
</span></span><span style="display:flex;"><span>nltk<span style="color:#f92672">.</span>download(<span style="color:#e6db74">&#39;gutenberg&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> nltk.corpus <span style="color:#f92672">import</span> gutenberg
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>texts <span style="color:#f92672">=</span> gutenberg<span style="color:#f92672">.</span>raw(<span style="color:#e6db74">&#39;shakespeare-hamlet.txt&#39;</span>)
</span></span></code></pre></div><h2 id="데이터-전처리">데이터 전처리<a hidden class="anchor" aria-hidden="true" href="#데이터-전처리">#</a></h2>
<p>데이터를 전처리하여 모델 학습에 적합한 형태로 변환합니다. 여기에는 텍스트 토큰화와 시퀀스 생성이 포함됩니다:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.preprocessing.text <span style="color:#f92672">import</span> Tokenizer
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.preprocessing.sequence <span style="color:#f92672">import</span> pad_sequences
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 텍스트 토큰화</span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> Tokenizer()
</span></span><span style="display:flex;"><span>tokenizer<span style="color:#f92672">.</span>fit_on_texts([texts])
</span></span><span style="display:flex;"><span>total_words <span style="color:#f92672">=</span> len(tokenizer<span style="color:#f92672">.</span>word_index) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 시퀀스 생성</span>
</span></span><span style="display:flex;"><span>input_sequences <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> texts<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>):
</span></span><span style="display:flex;"><span>    token_list <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>texts_to_sequences([line])[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, len(token_list)):
</span></span><span style="display:flex;"><span>        n_gram_sequence <span style="color:#f92672">=</span> token_list[:i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        input_sequences<span style="color:#f92672">.</span>append(n_gram_sequence)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 시퀀스 패딩</span>
</span></span><span style="display:flex;"><span>max_sequence_len <span style="color:#f92672">=</span> max([len(x) <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> input_sequences])
</span></span><span style="display:flex;"><span>input_sequences <span style="color:#f92672">=</span> pad_sequences(input_sequences, maxlen<span style="color:#f92672">=</span>max_sequence_len, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;pre&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 특징과 레이블 분리</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>input_sequences <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(input_sequences)
</span></span><span style="display:flex;"><span>X, y <span style="color:#f92672">=</span> input_sequences[:,:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], input_sequences[:,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 레이블 원핫 인코딩</span>
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>to_categorical(y, num_classes<span style="color:#f92672">=</span>total_words)
</span></span></code></pre></div><h2 id="모델-구성">모델 구성<a hidden class="anchor" aria-hidden="true" href="#모델-구성">#</a></h2>
<p>간단한 신경망 모델을 사용하여 텍스트 자동 완성 모델을 구성합니다:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.models <span style="color:#f92672">import</span> Sequential
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.layers <span style="color:#f92672">import</span> Embedding, LSTM, Dense
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> Sequential([
</span></span><span style="display:flex;"><span>    Embedding(total_words, <span style="color:#ae81ff">64</span>, input_length<span style="color:#f92672">=</span>max_sequence_len<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>    LSTM(<span style="color:#ae81ff">100</span>),
</span></span><span style="display:flex;"><span>    Dense(total_words, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>, loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;categorical_crossentropy&#39;</span>, metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])
</span></span></code></pre></div><h2 id="모델-훈련">모델 훈련<a hidden class="anchor" aria-hidden="true" href="#모델-훈련">#</a></h2>
<p>모델을 훈련시킵니다:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>history <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>fit(X, y, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><h2 id="텍스트-자동-완성-함수">텍스트 자동 완성 함수<a hidden class="anchor" aria-hidden="true" href="#텍스트-자동-완성-함수">#</a></h2>
<p>훈련된 모델을 사용하여 텍스트 자동 완성 기능을 구현합니다:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict_next_word</span>(model, tokenizer, text, max_sequence_len):
</span></span><span style="display:flex;"><span>    token_list <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>texts_to_sequences([text])[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    token_list <span style="color:#f92672">=</span> pad_sequences([token_list], maxlen<span style="color:#f92672">=</span>max_sequence_len<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;pre&#39;</span>)
</span></span><span style="display:flex;"><span>    predicted <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(token_list, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    predicted_word_index <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(predicted, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    predicted_word <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>index_word[predicted_word_index[<span style="color:#ae81ff">0</span>]]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> predicted_word
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>seed_text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;To be or not to be&#34;</span>
</span></span><span style="display:flex;"><span>next_word <span style="color:#f92672">=</span> predict_next_word(model, tokenizer, seed_text, max_sequence_len)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Seed text: &#39;</span><span style="color:#e6db74">{</span>seed_text<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;, Next word: &#39;</span><span style="color:#e6db74">{</span>next_word<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;&#34;</span>)
</span></span></code></pre></div><h2 id="전체-코드-예제">전체 코드 예제<a hidden class="anchor" aria-hidden="true" href="#전체-코드-예제">#</a></h2>
<p>아래는 위의 모든 단계를 포함한 전체 코드 예제입니다:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> nltk
</span></span><span style="display:flex;"><span>nltk<span style="color:#f92672">.</span>download(<span style="color:#e6db74">&#39;gutenberg&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> nltk.corpus <span style="color:#f92672">import</span> gutenberg
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow <span style="color:#66d9ef">as</span> tf
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.preprocessing.text <span style="color:#f92672">import</span> Tokenizer
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.preprocessing.sequence <span style="color:#f92672">import</span> pad_sequences
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.models <span style="color:#f92672">import</span> Sequential
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.layers <span style="color:#f92672">import</span> Embedding, LSTM, Dense
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 데이터 준비</span>
</span></span><span style="display:flex;"><span>texts <span style="color:#f92672">=</span> gutenberg<span style="color:#f92672">.</span>raw(<span style="color:#e6db74">&#39;shakespeare-hamlet.txt&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 텍스트 토큰화</span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> Tokenizer()
</span></span><span style="display:flex;"><span>tokenizer<span style="color:#f92672">.</span>fit_on_texts([texts])
</span></span><span style="display:flex;"><span>total_words <span style="color:#f92672">=</span> len(tokenizer<span style="color:#f92672">.</span>word_index) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 시퀀스 생성</span>
</span></span><span style="display:flex;"><span>input_sequences <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> texts<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>):
</span></span><span style="display:flex;"><span>    token_list <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>texts_to_sequences([line])[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, len(token_list)):
</span></span><span style="display:flex;"><span>        n_gram_sequence <span style="color:#f92672">=</span> token_list[:i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        input_sequences<span style="color:#f92672">.</span>append(n_gram_sequence)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 시퀀스 패딩</span>
</span></span><span style="display:flex;"><span>max_sequence_len <span style="color:#f92672">=</span> max([len(x) <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> input_sequences])
</span></span><span style="display:flex;"><span>input_sequences <span style="color:#f92672">=</span> pad_sequences(input_sequences, maxlen<span style="color:#f92672">=</span>max_sequence_len, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;pre&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 특징과 레이블 분리</span>
</span></span><span style="display:flex;"><span>input_sequences <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(input_sequences)
</span></span><span style="display:flex;"><span>X, y <span style="color:#f92672">=</span> input_sequences[:,:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], input_sequences[:,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 레이블 원핫 인코딩</span>
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>to_categorical(y, num_classes<span style="color:#f92672">=</span>total_words)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 신경망 모델 구성</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> Sequential([
</span></span><span style="display:flex;"><span>    Embedding(total_words, <span style="color:#ae81ff">64</span>, input_length<span style="color:#f92672">=</span>max_sequence_len<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>    LSTM(<span style="color:#ae81ff">100</span>),
</span></span><span style="display:flex;"><span>    Dense(total_words, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>, loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;categorical_crossentropy&#39;</span>, metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 모델 훈련</span>
</span></span><span style="display:flex;"><span>history <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>fit(X, y, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 텍스트 자동 완성 함수</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict_next_word</span>(model, tokenizer, text, max_sequence_len):
</span></span><span style="display:flex;"><span>    token_list <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>texts_to_sequences([text])[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    token_list <span style="color:#f92672">=</span> pad_sequences([token_list], maxlen<span style="color:#f92672">=</span>max_sequence_len<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;pre&#39;</span>)
</span></span><span style="display:flex;"><span>    predicted <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(token_list, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    predicted_word_index <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(predicted, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    predicted_word <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>index_word[predicted_word_index[<span style="color:#ae81ff">0</span>]]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> predicted_word
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>seed_text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;To be or not to be&#34;</span>
</span></span><span style="display:flex;"><span>next_word <span style="color:#f92672">=</span> predict_next_word(model, tokenizer, seed_text, max_sequence_len)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Seed text: &#39;</span><span style="color:#e6db74">{</span>seed_text<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;, Next word: &#39;</span><span style="color:#e6db74">{</span>next_word<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;&#34;</span>)
</span></span></code></pre></div><p>이 코드는 간단한 텍스트 자동 완성 기능의 기본적인 구조를 제공합니다. 실제 기능의 성능을 높이기 위해서는 다양한 데이터 전처리 기법과 모델을 적용할 수 있습니다.</p>
<h2 id="마무리">마무리<a hidden class="anchor" aria-hidden="true" href="#마무리">#</a></h2>
<p>이번 글에서는 Python과 자연어 처리를 사용하여 간단한 텍스트 자동 완성 기능을 구현하는 방법을 소개했습니다. 텍스트 자동 완성 기능은 다양한 애플리케이션에서 유용하게 사용될 수 있습니다. 다음 포스트에서는 조금 더 심화된 내용인 &lsquo;Python과 OpenCV를 이용한 실시간 객체 추적&rsquo;을 다뤄보도록 하겠습니다.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://funapps.site/posts/realtime_object_tracking/">
    <span class="title">« Prev</span>
    <br>
    <span>Python과 OpenCV를 이용한 실시간 객체 추적</span>
  </a>
  <a class="next" href="https://funapps.site/posts/ai_autonomous_simulation/">
    <span class="title">Next »</span>
    <br>
    <span>AI 기반의 자율주행 시뮬레이션 만들기: 초보자 가이드</span>
  </a>
</nav>

  </footer><div id="disqus_thread"></div>
<script>
    

    

    (function() { 
    var d = document, s = d.createElement('script');
    s.src = 'https://funapps-1.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://funapps.site/">펀앱스</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
